# -*- coding: utf-8 -*-
"""karantakhtani_mlassignment8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EoH0gU_-4JPZQrSMGyAV5F6P9awlAb-g
"""

import numpy as np
import pandas as pd

# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

df=pd.read_csv(r'HR.csv')

df.head()

"""# Data Exploratory Analysis

### 1.See datatypes and check for null Entry
### 2. See categorical columns and convert to numerical if possible
"""

df.describe()

df.info()

print(df['Over18'].unique())
print(df['EmployeeCount'].unique())
print(df['EmployeeNumber'].unique(), len(df['EmployeeNumber'].unique()))
print(df['StandardHours'].unique())

"""#Since the above shown columns contain same data ddor every row i.e. example so they do not affect the prediction"""

df.drop('EmployeeCount',axis=1,inplace=True)
df.drop('EmployeeNumber',axis=1,inplace=True)
df.drop('Over18',axis=1,inplace=True)
df.drop('StandardHours', axis=1, inplace=True)

df.select_dtypes(['object'])

X=df.drop('Attrition',axis=1)
y=df['Attrition']

X.shape

y.shape

plt.figure(figsize=(18,15))
sns.heatmap(df.corr(),annot=False)

# cannot deduce anything

"""## Using pandas.get_dummies
### To conver object to numerical data

imd_BusinessTravel=pd.get_dummies(df['BusinessTravel'],prefix='BusinessTravel',drop_first=True)
imd_Department=pd.get_dummies(df['Department'],prefix='Department',drop_first=True)
imd_EducationField=pd.get_dummies(df['EducationField'],prefix='EducationField',drop_first=True)
imd_Gender = pd.get_dummies(df['Gender'], prefix='Gender',drop_first=True)
imd_JobRole = pd.get_dummies(df['JobRole'], prefix='JobRole',drop_first=True)
imd_MaritalStatus = pd.get_dummies(df['MaritalStatus'], prefix='MaritalStatus',drop_first=True)
imd_OverTime = pd.get_dummies(df['OverTime'], prefix='OverTime',drop_first=True)
"""

# df1=pd.concat([df['BusinessTravel'],df['Department'],df['EducationField'],df['Gender'],df['JobRole'],df['MaritalStatus'],df['OverTime'],df.select_dtypes(['int64'])], axis=1)

# df1.head()

# df['Department']

df1=X

df1.head()

df1.shape

sns.factorplot(data=df,x='Attrition',y='Age',kind='box')

sns.factorplot(data=df,col='Department',x='Attrition',kind='count')

"""# Encoding

Converting target variable in numerical format
"""

from sklearn.preprocessing import LabelEncoder

le=LabelEncoder()
y_std=le.fit_transform(df['Attrition'])
print(le.classes_)

y_std

df1.columns

!pip install missingno

import missingno as msno

msno.matrix(df1)  # check for missing or null entry

"""# Check Categorical dataset and apply Label Encoder wherever possible"""

df1.info()

cat_col=df1.select_dtypes(['object'])
cat_col.columns

"""# Encoder for X

Converting independent variable to numerical format
"""

def transform(feature):
    df1[feature]=le.fit_transform(df1[feature])
    print(le.classes_)

for col in cat_col.columns:
    transform(col)

df1.head()

"""## Categorical columns now have been transformed to numerical

The Machine Learning model can now understand the data and can make hypothesis
"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(df1,y,test_size=0.3)

"""# Import models, techniques

Models:- SVM, Desicion Tree, Gradient boost classifier, Logistic regression etc.

techniques:- gridSearchCV, K-Fold, Accuray Metrics etc.
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.metrics import roc_auc_score,roc_curve,accuracy_score,precision_score,confusion_matrix,recall_score
from sklearn.preprocessing import StandardScaler

from sklearn.metrics import classification_report

from sklearn.model_selection import cross_val_score,cross_val_predict

scalar = StandardScaler()
scaled_df1=scalar.fit_transform(df1)

X_train,X_test,y_train,y_test=train_test_split(scaled_df1,y_std,test_size=0.3)

"""# Collecting accuracy scores

** Create list objects for all parameters and append it iteratively **

acc=[]
prec=[]
rec=[]
auroc=[]
"""

acc=[]
prec=[]
rec=[]
auroc=[]

"""# Defining function for calculating accuracy"""

def print_score(clf, X_train, y_train, X_test, y_test, train=True):
    '''
    print the accuracy score, classification report and confusion matrix of classifier
    '''
    if train:
        '''
        training performance
        '''
        print("Train Result:\n")
        print("accuracy score: {0:.4f}\n".format(accuracy_score(y_train, clf.predict(X_train))))
        print("Classification Report: \n {}\n".format(classification_report(y_train, clf.predict(X_train))))
        print("Confusion Matrix: \n {}\n".format(confusion_matrix(y_train, clf.predict(X_train))))

        res = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')
        print("Average Accuracy: \t {0:.4f}".format(np.mean(res)))
        print("Accuracy SD: \t\t {0:.4f}".format(np.std(res)))
        
    elif train==False:
        '''
        test performance
        '''
        print("Test Result:\n")        
        print("accuracy score: {0:.4f}\n".format(accuracy_score(y_test, clf.predict(X_test))))
        print("Classification Report: \n {}\n".format(classification_report(y_test, clf.predict(X_test))))
        print("Confusion Matrix: \n {}\n".format(confusion_matrix(y_test, clf.predict(X_test))))

"""# Support Vector Classifier"""

scvm_vlf=SVC()

params_grid = {"C": [1,2,3,4,5,6,31],
               "kernel": ['rbf','linear','poly'],
               "degree": [1,2, 3,4]
              }

grid_search1 = GridSearchCV(scvm_vlf, params_grid,
                           n_jobs=-1,
                           verbose=1)

grid_search1.fit(X_train,y_train)

grid_search1.best_score_

grid_search1.best_estimator_.get_params()

scv_clf =SVC(C= 3,degree= 1,kernel= 'rbf')

scv_clf.fit(X_train,y_train)

y_p1=scv_clf.predict(X_test)

acc.append(accuracy_score(y_test,y_p1))
    prec.append(precision_score(y_test,y_p1))
    rec.append(recall_score(y_test,y_p1))
    auroc.append(roc_auc_score(y_test,y_p1))

acc

prec

confusion_matrix(y_test,y_p1)

print_score(scv_clf,X_train, y_train, X_test, y_test, train=True)

print_score(scv_clf,X_train, y_train, X_test, y_test, train=False)

"""# Descion Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier

clf_dt_temp=DecisionTreeClassifier()

params_grid_dt = {"criterion" : ['gini','entropy'],
                  "max_leaf_nodes" : list(range(2,75)),
                  "min_samples_split" : [2,3,4],
                  "min_samples_leaf" : list(range(5,50))
    
}

grid_search2=GridSearchCV(clf_dt_temp,params_grid_dt,n_jobs=-1,verbose=1)

grid_search2.fit(X_train,y_train)

grid_search2.best_score_

grid_search2.best_estimator_.get_params()

clf_dt=DecisionTreeClassifier(criterion='entropy',max_leaf_nodes=8,min_samples_leaf=24,min_samples_split=2)

clf_dt.fit(X_train,y_train)

y_p2=clf_dt.predict(X_test)

acc.append(accuracy_score(y_test,y_p2))
    prec.append(precision_score(y_test,y_p2))
    rec.append(recall_score(y_test,y_p2))
    auroc.append(roc_auc_score(y_test,y_p2))

acc

print_score(clf_dt,X_train,y_train,X_test,y_test,train=True)

print_score(clf_dt,X_train,y_train,X_test,y_test,train=False)

"""# Logistic Regression"""

clf_lr_temp=LogisticRegression()

params_grid_lr1={"penalty" : ['l2'],
                "C" : [0.001,0.009,0.01,0.09,0.1,0.9,1],
                "solver" : ['newton-cg', 'lbfgs', 'sag','saga']
    
}

grid_search3=GridSearchCV(clf_lr_temp,params_grid_lr1,n_jobs=-1,verbose=1)

grid_search3.fit(X_train,y_train)

grid_search3.best_score_

grid_search3.best_estimator_.get_params()

params_grid_lr2={"penalty" : ['l1'],
                "C" : [0.001,0.009,0.01,0.09,0.1,0.9,0.8,0.95,1],
                "solver" : ['liblinear', 'saga']
                                                }

grid_search4=GridSearchCV(clf_lr_temp,params_grid_lr2,n_jobs=-1)

grid_search4.fit(X_train,y_train)

grid_search4.best_score_

grid_search4.best_estimator_.get_params()

clf_lr=LogisticRegression(C=0.09,penalty='l2',solver='newton-cg')

clf_lr.fit(X_train,y_train)

y_p3=clf_lr.predict(X_test)

acc.append(accuracy_score(y_test,y_p2))
    prec.append(precision_score(y_test,y_p2))
    rec.append(recall_score(y_test,y_p2))
    auroc.append(roc_auc_score(y_test,y_p2))

d={'Modelling Algo':['SVC','Desicion Tree','Logistic Regression'],'Accuracy':acc,'Precision':prec,'Recall':rec,'Area Under ROC Curve':auroc}
met_df=pd.DataFrame(d)
met_df

